{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23779186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "from string import Template\n",
    "import re\n",
    "import pypandoc\n",
    "\n",
    "openai.api_key_path = '/home/tim/projects/openai/apikey.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078878db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which completion model to use?  \n",
    "\n",
    "MODELS = {\n",
    "    \"something_else\":'',\n",
    "    \"text1\": \"text-davinci-003\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dad9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at me prompt engineering.\n",
    "\n",
    "topics = Template(\"\"\"\n",
    "I'm going to ask you to summarize a meeting transcript.  The transcript will contain a marker for each speaker's \n",
    "utterance, which you can ignore.  \n",
    "\n",
    "Here's an eample of a marker for a speaker named Tim Dolbeare:\n",
    "\n",
    "0:1:47.940 --> 0:1:48.410\n",
    "Tim Dolbeare\n",
    "\n",
    "Where's an example of a marker for a speaker named Lydia Ng:\n",
    "\n",
    "0:1:26.950 --> 0:1:41.660\n",
    "David Feng\n",
    "\n",
    "Please start your summary with a bulleted list of topics, and then a second bulleted containing a summary of each topic.\n",
    "\n",
    "Here's the transcript:\n",
    "    \n",
    "$q   \n",
    "\"\"\")\n",
    "\n",
    "summary = Template(\"\"\"\n",
    "I'm going to ask you to summarize a meeting transcript.  In the transcript each speaker's utterance begins with \n",
    "two new line characters, and then their name.  \n",
    "\n",
    "Here's the transcript:\n",
    "    \n",
    "$q   \n",
    "\"\"\")\n",
    "\n",
    "super_sum = Template(\"\"\"\n",
    "I'm going to ask you to summarize a collection of meeting summary excerpts. All of the excerpts are \n",
    "from the same meeting, and have been concatenated together below.\n",
    "\n",
    "Please structure your response as--\n",
    "- a summarized list of important topics discussed\n",
    "- for each topic, a summary of the discussion in one or two sentences\n",
    "- Finally, a list of any actions decided upon.\n",
    "\n",
    "Here's the collection of summaries:\n",
    "    \n",
    "$q   \n",
    "\"\"\")\n",
    "\n",
    "topic_summary = Template(\"\"\"\n",
    "I'm going to ask you to summarize a collection of meeting summary excerpts. All of the excerpts are \n",
    "from the same meeting, and have been concatenated together.\n",
    "\n",
    "Your job is to produce a list of important topics discussed in the meeting.  Please try to remove any \n",
    "redundancy from your final list of topics, so that each subject appears only once in the topic list, even if it \n",
    "was discussed multiple times.\n",
    "\n",
    "Here's the collection of summaries:\n",
    "    \n",
    "$q   \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "general_question = Template(\"\"\"\n",
    "Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: $q\n",
    "\"\"\")\n",
    "                   \n",
    "\n",
    "free = Template(\"\"\"\n",
    "$q\n",
    "\"\"\")\n",
    "\n",
    "PROMPTS = {\n",
    "    \"topics\": topics,\n",
    "    \"summary\": summary,\n",
    "    \"general\": general_question,\n",
    "    \"super_sum\": super_sum,\n",
    "    \"topic_summary\": topic_summary,\n",
    "    \"free\": free\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13bf07",
   "metadata": {},
   "source": [
    "### What Do?\n",
    "* Convert docx to txt\n",
    "* Pre-process txt to remove chaff\n",
    "* chunk and summarize\n",
    "  * chunk txt into summarizeable pieces\n",
    "  * summarize each chunk\n",
    "  * combine summaries\n",
    "  \n",
    "#### To Do\n",
    "* hierachical summarizing for really long transcripts\n",
    "* estimate cost of summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5a1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_text(word_file):\n",
    "    path, name = os.path.split(word_file)\n",
    "    name = os.path.splitext(name)[0]\n",
    "    txt_name = name+'.txt'\n",
    "    outfile = os.path.join(path, txt_name)\n",
    "    \n",
    "    pypandoc.convert_file(word_file, 'plain', outputfile=outfile)\n",
    "    return outfile, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d50465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript(file_name):\n",
    "    with open(file_name) as f:\n",
    "        return f.read()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617b0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transcript(transcript):\n",
    "    # remove timestamps\n",
    "    temp = re.sub(\"[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{1,3} --> [0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{1,3}\\n\", '', transcript)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a5074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200):\n",
    "    utterances = text.split('\\n\\n')\n",
    "    total = len(utterances)\n",
    "    #total = len(text)\n",
    "    chunk_count = (total//chunk_size) + 1\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0,chunk_count):\n",
    "        start = i*chunk_size\n",
    "        end = min(start+chunk_size, total)\n",
    "        chunks.append(utterances[start:end])\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c165ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a prompt and a question, return a translation from NL.\n",
    "# The key parameter that we're varying in his notebook is translation_type, which\n",
    "# selects for one of the prompts designed above.\n",
    "\n",
    "def submit_prompt(q, translation_type, completion_model='text1', temp=0, max_tokens=900):\n",
    "    \n",
    "    prompt = PROMPTS[translation_type]\n",
    "    \n",
    "    r = openai.Completion.create(\n",
    "        prompt=prompt.substitute({'q': q}),\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens,\n",
    "        model=MODELS[completion_model]\n",
    "    )[\"choices\"][0][\"text\"].strip(\" \\n\")       \n",
    "    r = r.replace('A:', '').strip()\n",
    "    \n",
    "    return f\"\\n{r}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, prompt='summary', max_tokens=900):\n",
    "    r = ''\n",
    "    for chunk in chunks:\n",
    "        r += submit_prompt(chunk, prompt, max_tokens=max_tokens)\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7885924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(word_file, chunk_size=100, max_tokens=500):\n",
    "    \n",
    "    transcript_file, file_label = word_to_text(word_file)\n",
    "    raw_transcript = load_transcript(transcript_file)\n",
    "    preprocessed = preprocess_transcript(raw_transcript)\n",
    "    chunks = chunk_text(preprocessed, chunk_size=chunk_size)\n",
    "\n",
    "    sub_sums = summarize_chunks(chunks, prompt='summary', max_tokens=max_tokens)    \n",
    "    sum_sum = submit_prompt(sub_sums, 'super_sum', temp=0, max_tokens=1000)\n",
    "    \n",
    "    return {\n",
    "        \"steps\": sub_sums,\n",
    "        \"summary\": sum_sum,\n",
    "        \"title\": file_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82e44dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meeting: CodeOcean&AWSOPS2023-02-15\n",
      "\n",
      "\n",
      "Important Topics Discussed:\n",
      "- Features of Code Ocean\n",
      "- Use of language models to create summaries\n",
      "- Use of Code Ocean for AIBS scientists\n",
      "- Hosting Python or R\n",
      "- Advantages of using Code Ocean\n",
      "- Cost of using Code Ocean\n",
      "- Production workflows\n",
      "- Criteria for determining when to use Code Ocean\n",
      "- Analysis or position paper to make an informed decision\n",
      "- Connecting with an AWS solution architect\n",
      "- Presentation from June of last year by David\n",
      "- Risk of Code Ocean going under\n",
      "\n",
      "Summaries of Discussions:\n",
      "- Shoaib Mufti and Tim Dolbeare discussed the potential of using Code Ocean to manage data science projects, including the features of Code Ocean, such as the ability to create capsules that encapsulate the compute environment, the use of S3 as storage, and the no-code workflow system. \n",
      "- Tim Dolbeare discussed how Code Ocean can be used to host Python or R, and how administrators can define what machines are available to users. \n",
      "- Rob and Tim discussed the use of Code Ocean for production workflows, and agreed that the scientists should not be building production workflows, but rather the engineers should take possession of the workflows and optimize them for cost and speed. \n",
      "- Tyler Mollenkopf provided details about a presentation from June of last year by David, which included a visual of the workflow and ratings of the six jobs that the platform wanted to accomplish. \n",
      "- Shoaib Mufti suggested getting early bias and opinion from AWS. \n",
      "\n",
      "Actions Decided Upon:\n",
      "- Get names and a draft of an updated version of David's presentation. \n",
      "- Test the theory by running a workflow for a week and comparing the cost and performance of using Code Ocean versus optimizing the workflow. \n",
      "- Get early bias and opinion from AWS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = create_summary('/home/tim/work/projects/transcripts/CodeOcean&AWSOPS2023-02-15.docx')\n",
    "print('\\nMeeting: '+s['title']+'\\n')\n",
    "print(s['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dac3052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meeting: AWS_LZ_Fun_Time_2023-02-10\n",
      "\n",
      "\n",
      "Summary of Important Topics Discussed:\n",
      "- Need to create a transcript in the meeting\n",
      "- Use cases for identity management\n",
      "- Possibility of using AWS Identity Center and Cognito to manage user groups\n",
      "- Need to be able to flexibly bring people in or out of a service on top of AWS\n",
      "- Possibility of using attributes to provide different roles for different projects\n",
      "- Need to be able to share with collaborators and partition access to resources within a cloud\n",
      "- Budget in the app and how to access the bucket\n",
      "- Credentials needed to access plain vanilla S3 buckets\n",
      "- Solutions that would support B2C tenant and open ID connect\n",
      "- Challenges of managing user access to different applications\n",
      "- Possibility of using an identity provider to manage access\n",
      "- Possibility of federating with larger institutions to manage access\n",
      "- Possibility of using Azure AD as an abstraction layer to integrate with different applications\n",
      "- Need for a single identity provider that could collaborate across different organizations\n",
      "- Need for different levels of access, such as read-only, read-write, and delete\n",
      "- Need for temporary credentials to access the S3 buckets\n",
      "- Need for a service account to manage the transfer of files\n",
      "- Need for a library to get the secret key and access the bucket\n",
      "- Need for provisioned buckets for specific studies\n",
      "- Need to automate the connection between users and projects managed in a database\n",
      "- Need to assign resources or access to S3 buckets\n",
      "- Possibility of providing an abstraction layer that looks like a file system\n",
      "- Possibility of using AWS Transfer Family, an SFTP server that connects to an S3 bucket\n",
      "\n",
      "Summaries of Discussions:\n",
      "- Tim Dolbeare and Rob Young discussed the need to create a transcript in the meeting, and David Feng mentioned that he had been asked to summarize a document.\n",
      "- David Feng then explained his need to be able to flexibly bring people in or out of a service on top of AWS, and the need for an API that is not currently supported by AWS.\n",
      "- David Aiken discussed the budget in the app and how to access the bucket, and David Feng and Tim Dolbeare discussed the credentials needed to access plain vanilla S3 buckets.\n",
      "- They discussed the solutions that would support B2C tenant and open ID connect.\n",
      "- They discussed the challenges of managing user access to different applications, such as AWS, Azure, and Code Ocean.\n",
      "- They discussed the need for a single identity provider that could collaborate across different organizations.\n",
      "- They discussed the need for different levels of access, such as read-only, read-write, and delete, and the need for temporary credentials to access the S3 buckets.\n",
      "- They discussed the need for a service account to manage the transfer of files and the need for a library to get the secret key and access the bucket.\n",
      "- They discussed the need for provisioned buckets for specific studies and the need to manage resource access to those resources.\n",
      "- They discussed the need to automate the connection between users and projects managed in a database, and the need to assign resources or access to S3 buckets.\n",
      "- They discussed the possibility of providing an abstraction layer that looks like a file system, and the possibility of using AWS Transfer Family, an SFTP server that connects to an S3 bucket.\n",
      "\n",
      "Actions Decided Upon:\n",
      "- Brainstorm and come back with proposals or designs for managing user access to different applications.\n",
      "- Brainstorm and come back with proposals or designs for providing an abstraction layer that looks like a file system.\n",
      "- Brainstorm and come back with proposals or designs for using AWS Transfer Family, an SFTP server that connects to an S3 bucket.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = create_summary('/home/tim/work/projects/transcripts/AWS_LZ_Fun_Time_2023-02-10.docx')\n",
    "print('\\nMeeting: '+s['title']+'\\n')\n",
    "print(s['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3851993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meeting: Teams-enabledTapeBackupandDataWarehouseDeepDive _2023-02-17\n",
      "\n",
      "\n",
      "Important topics discussed:\n",
      "- Legacy web applications of the Allen Institute\n",
      "- Containerizing the spinal cord Atlas in a Kubernetes\n",
      "- Moving the image service to object storage\n",
      "- Backup and archive system\n",
      "- Software development project from 2000\n",
      "- Upgrading the Postgres 9.4 version migration\n",
      "- Apache configuration\n",
      "- Two generations of the image service\n",
      "- Source code for the old one\n",
      "- Cloud Optimized Geotiff format\n",
      "- FSX for Lustre\n",
      "- Data ingestion\n",
      "- Tape backup system\n",
      "- Redirecting what is currently going to tape into glacier\n",
      "- Retrieving content from glacier\n",
      "- Digital Asset Tracking Service\n",
      "- Image viewer tool\n",
      "- Neural clancer\n",
      "- 3D Slicer image\n",
      "- Data governance\n",
      "- Disaster recovery\n",
      "- Backup\n",
      "- Deleting data sets\n",
      "- Next steps for dealing with LIMS\n",
      "\n",
      "Summary of topics discussed:\n",
      "- Rob Young and Tim Dolbeare discussed the legacy web applications of the Allen Institute, which have not been migrated to the data warehouse. Brian Youngstrom has done some preliminary work to containerize the spinal cord Atlas in a Kubernetes, but the other applications are hosted on the colo Isilon. \n",
      "- Tim Dolbeare and weirnich discussed the image service and how it could be moved to object storage. They discussed the Cloud Optimized Geotiff format and other potential solutions, such as FSX for Lustre, which uses S3 buckets and a virtual POSIX compliant file system. They also discussed the backup and archive system, which is already in AWS, and the Apache configuration that will need to be taken into account when migrating the application. \n",
      "- Rob Young and Brian Youngstrom discussed the possibility of upgrading the Postgres 9.4 version migration, but they were worried about the RMA API that was built. Jose Melchor mentioned that they use Sphinx, and James Jeyaprakash suggested using Aurora for better performance and high availability. \n",
      "- Brian Youngstrom discussed the spinal cord analysis as a potential first target, and the challenges of using an old version of Ruby. Rob Young and Jose Melchor discussed the use of the warehouse and the process of data ingestion. \n",
      "- Tim Dolbeare discussed a software development project from 2000 that had gone through several hands. \n",
      "- Brian Youngstrom asked about the process of restoring content from the tape archive, and James Jeyaprakash (Guest) explained that it would take 1-3 hours to retrieve a file from the glacier. Rob Young discussed the Digital Asset Tracking Service, which tracks files on the Isilon and S3 buckets, and the possibility of using a gateway to access the files. \n",
      "- Jose Melchor mentioned the image viewer tool, which is tightly coupled with LIMS 2, and the possibility of converting the image files to a format that would be easier to access. \n",
      "- Brian Youngstrom provided information about the amount of data that has been backed up in the existing tape storage, which is over 10 petabytes across 3 generations of tape. They also discussed data governance, disaster recovery, and backup, as well as the possibility of deleting data sets. \n",
      "- Rob Young and weirnich discussed the next steps for dealing with LIMS and the deep dives they need to do. \n",
      "\n",
      "Actions decided upon:\n",
      "- Stop putting new stuff on tape and get it into glacier\n",
      "- Figure out what to do with the old content later\n",
      "- Pilot the migration with the adult mouse brain\n",
      "- Do an inventory of the different use cases\n",
      "- Have a data governance conversation\n",
      "- Do deep dives into LIMS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = create_summary('/home/tim/work/projects/transcripts/Teams-enabledTapeBackupandDataWarehouseDeepDive_2023-02-17.docx')\n",
    "print('\\nMeeting: '+s['title']+'\\n')\n",
    "print(s['summary'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
