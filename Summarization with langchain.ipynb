{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfd4ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#import openai\n",
    "#from string import Template\n",
    "#import re\n",
    "#import pypandoc\n",
    "\n",
    "#openai.api_key_path = '/home/tim/projects/openai/apikey.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "063c9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = OpenAI(temperature=0, max_tokens=-1)\n",
    "llm = OpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0.5)\n",
    "llm.max_tokens\n",
    "#llm.max_dec_steps\n",
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cca04650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tim/work/projects/transcripts/AWS_LZ_Fun_Time_2023-02-10.txt') as f:\n",
    "    state_of_the_union = f.read()\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7e22afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e405b63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d78a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bdb24156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function langchain.chains.summarize.load_summarize_chain(llm: langchain.llms.base.BaseLLM, chain_type: str = 'stuff', verbose: Optional[bool] = None, **kwargs: Any) -> langchain.chains.combine_documents.base.BaseCombineDocumentsChain>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f7c9b72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RefineDocumentsChain\nprompt\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m meet_prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWrite a detailed summary of the following exceprt from a meeting transcript. Begin with a bulleted\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mlist of topics discussed.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;132;01m{text}\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m meet_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mprompt_template, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_summarize_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrefine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeet_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m chain\u001b[38;5;241m.\u001b[39mrun(docs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/summarize/__init__.py:137\u001b[0m, in \u001b[0;36mload_summarize_chain\u001b[0;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     )\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/summarize/__init__.py:99\u001b[0m, in \u001b[0;36m_load_refine_chain\u001b[0;34m(llm, question_prompt, refine_prompt, document_variable_name, initial_response_name, refine_llm, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m _refine_llm \u001b[38;5;241m=\u001b[39m refine_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m     98\u001b[0m refine_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39m_refine_llm, prompt\u001b[38;5;241m=\u001b[39mrefine_prompt, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRefineDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_llm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefine_llm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefine_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_response_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_response_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:342\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RefineDocumentsChain\nprompt\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "meet_prompt_template = \"\"\"Write a detailed summary of the following exceprt from a meeting transcript. Begin with a bulleted\n",
    "list of topics discussed.\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "meet_prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", prompt=meet_prompt)\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "789530fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot stream results with multiple prompts.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#, return_intermediate_steps=False, map_prompt=map_prompt, combine_prompt=combine_prompt)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#outs = chain({\"input_documents\": docs}, return_only_outputs=True)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/base.py:239\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/base.py:142\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    135\u001b[0m     inputs,\n\u001b[1;32m    136\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:56\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     55\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m---> 56\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:139\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m, docs: List[Document], token_max: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_results(results, docs, token_max, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/llm.py:117\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_list: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124;03m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/llm.py:59\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 59\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/llms/base.py:128\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/llms/base.py:125\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    122\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.10/site-packages/langchain/llms/openai.py:247\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot stream results with multiple prompts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     response \u001b[38;5;241m=\u001b[39m _streaming_response_template()\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot stream results with multiple prompts."
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a detailed summary of the following exceprt from a meeting transcript.  \n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt_template = \"\"\"Write a detailed summary of the following meeting notes.  A detailed summary should \n",
    "include at least two paragraphs.\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")#, return_intermediate_steps=False, map_prompt=map_prompt, combine_prompt=combine_prompt)\n",
    "#outs = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9aa20816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This meeting was held to discuss the possibility of using AWS Identity Center to manage user groups for Code Ocean, a service on top of AWS that allows scientists to do analysis on data in the cloud. David Aiken and Rob Young discussed the limitations of the support they had received from Code Ocean and Rob suggested using Cognito in conjunction with it to enable the pushing of user groups. Tim Dolbeare then explained his use case, which was similar to David Aiken's, and asked if they could continue using their existing services or if they would need to rework the model in their applications. David Feng then explained that Code Ocean has a lightweight integrated identity management system, but they recommend using Octa for anything nontrivial. Rob Young then suggested using attributes as one way to accomplish the goal of the meeting, but noted that this would require storing the attributes at the identity provider, which would be a nonstarter. Tim Dolbeare then added that when in Code Ocean, users can access plain vanilla S3 buckets to pull data in. \n",
      "\n",
      "At the end of the meeting, Tim Dolbeare suggested creating a transcript of the meeting and Rob Young mentioned a new technology that can summarize documents and put them in an executive summary. David Feng then suggested starting on the\n"
     ]
    }
   ],
   "source": [
    "print(outs['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b532d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Verse 1\n",
      "I'm sippin' on sparkling water,\n",
      "It's so refreshing and light,\n",
      "It's the perfect way to quench my thirst\n",
      "On a hot summer night.\n",
      "\n",
      "Chorus\n",
      "Sparkling water, sparkling water,\n",
      "It's the best way to stay hydrated,\n",
      "It's so crisp and so clean,\n",
      "It's the perfect way to stay refreshed.\n",
      "\n",
      "Verse 2\n",
      "I'm sippin' on sparkling water,\n",
      "It's so bubbly and bright,\n",
      "It's the perfect way to cool me down\n",
      "On a hot summer night.\n",
      "\n",
      "Chorus\n",
      "Sparkling water, sparkling water,\n",
      "It's the best way to stay hydrated,\n",
      "It's so crisp and so clean,\n",
      "It's the perfect way to stay refreshed.\n",
      "\n",
      "Verse 3\n",
      "I'm sippin' on sparkling water,\n",
      "It's so light and so clear,\n",
      "It's the perfect way to keep me cool\n",
      "On a hot summer night.\n",
      "\n",
      "Chorus\n",
      "Sparkling water, sparkling water,\n",
      "It's the best way to stay hydrated,\n",
      "It's so crisp and so clean,\n",
      "It's the perfect way to stay refreshed."
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "llm = OpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
    "resp = llm(\"Write me a song about sparkling water.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ca87594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a50feafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tim Dolbeare suggested creating a transcript of the meeting and Rob Young mentioned a new technology that can summarize documents and put them in an executive summary. David Feng then suggested starting on the wiki page to discuss user stories and the different use cases for the app, such as controlling access to AWS resources and running applications and APIs that need identity. Rob Young then mentioned a chatbot called Chad GPT that has restrictions and Tim Dolbeare mentioned he was going to download the transcript and put it through a GPT 3 model to make notes.\n",
      "\n",
      "David Aiken and Rob Young are discussing the possibility of using AWS Identity Center to manage user groups for Code Ocean, a service on top of AWS that allows scientists to do analysis on data in the cloud. David needs to be able to flexibly bring people in or out of the service and provision user groups. Rob noted that David was sent an email from Code Ocean support expressing concern about the limitations of the support. Rob suggested that Cognito could be used in conjunction with it to enable the pushing of user groups. He also expressed concern about being able to provide different roles for different projects and associate those projects with data and databases. David agreed with Rob's suggestion.\n",
      "\n",
      "Rob Young suggested using attributes as one way to accomplish the goal of the meeting, but noted that this would require storing the attributes at the identity provider, which would be a nonstarter as anyone with access to the identity provider could add whatever projects they wanted. Tim Dolbeare then explained his use case, which was similar to David Aiken's, and asked if they could continue using their existing services or if they would need to rework the model in their applications. David Feng then explained that Code Ocean has a lightweight integrated identity management system, but if they wanted to do anything nontrivial, they would need to use an IDP like Octa. Aiken then clarified that access to Code Ocean does not necessarily mean access to an S3 bucket, and David Feng confirmed that the buckets do not have AWS credentials unless something special is done. Tim Dolbeare then added that when in Code Ocean, users can access plain vanilla S3 buckets to pull data in.\n"
     ]
    }
   ],
   "source": [
    "for step in outs['intermediate_steps']:\n",
    "    print(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
